{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3: Importing and manipulating data\n",
    "\n",
    "In the exercise, we're going to learn how to play with some tools for importing and maipulating data sets. We'll focus on a couple of key file types:\n",
    "\n",
    "* Hierarchical Data Format 5 (HDF5): A format for saving large data sets in a \"hierarchical\" format like folders on your computer. In addition to storing the data itself in an efficient way, this format allows for metadata about the datasets to be embedded in the file itself. [HDF5 website](https://www.hdfgroup.org)\n",
    "* Comma Seperated Value (CSV): Simply a spreadsheet stored as a text file.\n",
    "* JavasSript Object Notation (JSON): A format for saving key value pairs (akin to a Python dictionary). Can open it in any text editor like CSV. [JSON website](https://www.json.org/)\n",
    "\n",
    "Beyond talking about ways to open and access these files directly in python via the standard libraries and numpy, we'll also be talking about two really powerful tools for working with data in python:\n",
    "\n",
    "* Pandas: a fast, powerful, flexible and easy to use open source data analysis and manipulation tool (reproduces much of the functionality of R). [Pandas website](https://pandas.pydata.org)\n",
    "* Xarray:  an open source projectfor working with labelled multi-dimensional arrays. [Xarray website](http://xarray.pydata.org/en/stable/)\n",
    "\n",
    "One great place to get data is \n",
    "\n",
    "* [Collaborative Research in Computational Neuroscience - Data repository](crcns.org): Great resource for published data sets. Need to request access but it is open.  \n",
    "\n",
    "Good to practice with a complete data set _and_ see how other people store, organize, & archive their data.\n",
    "\n",
    "## Really important stuff we probably won't have time for\n",
    "\n",
    "* [Neurodata Without Borders (NWB)](https://www.nwb.org): New open standard for storing all kinds of neuroscience data that should be interoperable between tools. \n",
    "* The [Allen Brain Observatory software development kit (allensdk)](https://allensdk.readthedocs.io/en/latest/install.html) has a lot of really powerful tools as well as access to their data. v"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 labeled arrays using `xarray` <a name=\"xarray\">\n",
    "\n",
    "Very often arrays are objects that have coordinates associated with each of the dimesions (like time or space), but numpy doesn't have a method for storing those coordinates with the arrays. This can get cumbersome and basic operations (like interpolating between two coordinates) becomes more difficult than you would want. `xarray` is a package designed to address these deficiencies in `numpy`. Two main obects `DataArray` and a `DataSet`. \n",
    "\n",
    "[The docs for `xarray` can be found here](http://xarray.pydata.org/en/stable/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xarray as xr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.1 Constructing a `DataArray`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining some coordinates \n",
    "time = np.linspace(0, 20, 401) # in seconds\n",
    "space = np.linspace(-10, 10, 1001) # in centimeters\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining some function of space and time\n",
    "def height_of_wave(t, x, wavelength=2, speed=.1):\n",
    "    return(np.sin(2*np.pi*(x - speed*t) / wavelength))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creating arrays to input into function\n",
    "tv, sv = np.meshgrid(time, space)\n",
    "\n",
    "# calculating values of function at each value of `space` and `time`\n",
    "height = height_of_wave(tv, sv)\n",
    "height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "height.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Cheating a bit to make sure this works...\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "plt.imshow(height, cmap=\"jet\")\n",
    "plt.colorbar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, so now we have three objects that all go together\n",
    "1. The actual data: `height`\n",
    "2. An array representing the `x` coordinate `time`\n",
    "3. An array representing the `y` coordinate `space`\n",
    "\n",
    "**and** an implicit set of units `s` & `cm`. \n",
    "\n",
    "_Wouldn't be nice if there were a way to stroe these in one object? There is! `xr.DataArray`_\n",
    "\n",
    "The syntax for `DataArray` is\n",
    "\n",
    "`xr.DataArray(DATA, dims=TUPLE_OF_DIM_NAMES, coords=DICT_OF_COORDINATE_NAMES_AND_VALUES)`\n",
    "\n",
    "Note that \n",
    "\n",
    "1. The order of the `dims` matters. The first named dim corresponds to axis 0 in your array, the second corresponds to axis 1, etc\n",
    "2. the coords keyword takes a dictionary where the keys are the dims that have been named and the coordinate values as values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Did I do this right?\n",
    "\n",
    "xr_height = xr.DataArray(height, dims=(\"x\", \"t\"), coords={\"x\": space, \"t\": time})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Look how pretty the readout is when I ask jupyter about `xr_height`\n",
    "\n",
    "xr_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What are `attributes` all about? A dictionary where you can store whatever additional information you want.\n",
    "\n",
    "xr_height.attrs[\"year\"] = 2020"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xr_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`xarray` has two special variables that it uses:\n",
    "\n",
    "* `long_name`: a long name for the DataArray\n",
    "* `units`: self-explanatory but note that the DataArray _and_ the coordinates have seperate attributes where you can set each of their units. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting some key attributes\n",
    "\n",
    "xr_height.attrs[\"long_name\"] = \"Height of a travelling wave\"\n",
    "xr_height.attrs[\"units\"] = \"mm\"\n",
    "xr_height.x.attrs[\"units\"] = \"cm\"\n",
    "xr_height.t.attrs[\"units\"] = \"s\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check to see you can find the metadata...\n",
    "xr_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Accessing data array elements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having all the coordinate info included with the object means accessing relevant info is much easier. For example if we only had the original trio of numpy arrays and we wanted to find the value of the `height` from the array when `space` = 4 and `time` = 2 we would need to use all three objects: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Finding the value of `height` when `space` = 4 cm and `time` = 2 s\n",
    "\n",
    "height[space == 4, time == 2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "height[(space>0)&(space<4), time == 2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you had multiple arrays floating around with their own `space` and `time` this would get challenging fast.\n",
    "\n",
    "To access `DataArray` information you can use multiple methods.\n",
    "\n",
    "1. index the array by position and integer label (just like in numpy)\n",
    "2. `.isel` by dimentional name and integer label\n",
    "3. `.loc` by positional and coordinate label\n",
    "4. `.sel` by dimensional name and coordinate label"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Going back to the example before when `space` = 4 and `time` = 2.\n",
    "# What location is this in the array?\n",
    "\n",
    "print(np.where(space == 4))\n",
    "print(np.where(time == 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ok it is at the index integers 700, 40. So we could pick out that place and time from  `xr_height`\n",
    "# Should see the same value as above pop out. \n",
    "\n",
    "xr_height[700, 40]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now `.sel` also uses these integer indexes. Note: isel is a function included\n",
    "# with a `DataArray`. It takes keyword arguments that are the dim names.\n",
    "# Note that I can put these in any order I want and the reader can explicitly \n",
    "# see what 40 and 700 refer to. \n",
    "\n",
    "xr_height.isel(t=40, x=700)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Now the really cool part is skipping the whole step of finding the index and \n",
    "# just using the coordinate values themselves. Analgous to the above we have\n",
    "# two ways of doing this: without keywords and with. Without keywords is the\n",
    "# `.loc` function. Like traditional indexing it uses subscripting (i.e. [ ])\n",
    "\n",
    "xr_height.loc[4, 2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Similarly, `.sel` is a function allows you to specify the keywords explictly\n",
    "\n",
    "xr_height.sel(t=2, x=4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# You can even slice **using the coordinate values**. This slice takes all\n",
    "# values of x from 0 to 4 and all values of t from 0 to 2\n",
    "\n",
    "xr_height.loc[0:4, 0:2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.3 Interpolating "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What happens when you ask one of these magic coordinate methods \n",
    "# to pull a value that's not actually in the set of coordinates?\n",
    "\n",
    "xr_height.loc[4, np.pi]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# You can use loc to pick out the nearest value even if its not in the coords\n",
    "\n",
    "xr_height.sel(x=4.17263, t=np.pi, method=\"nearest\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ouch! Python gave us a `KeyError`. That's fair, after all our coordinates \n",
    "# didn't have that key. There's another function which will use surrounding \n",
    "# data and interpolate from that data to the point we want: `.interp` \n",
    "# it follows same conventions as `.sel` in that you have to specify the names \n",
    "# of the coordinates.\n",
    "\n",
    "xr_height.interp(x=4, t=np.pi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking to see if this is reasonable...\n",
    "\n",
    "xr_height.loc[4, 3.1:3.2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.4 Math with `DataArray` objects\n",
    "\n",
    "`DataArray` objects have many of the same standard functions `np.array` objects do, but knowledge of te coordinates makes life easier. We'll look at a few examples\n",
    "\n",
    "1. Univariate operations (like `.mean()`) on a index no longer require you to remember which variable was axis 0. You can just use the coordinate names to specify which dimension you want to averate over. \n",
    "2. Bivariate operations (like `*`) will automatically do what ever makes sense based on whether the axes line up or not! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Taking the overall mean works as before...\n",
    "\n",
    "xr_height.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Averaging over the x direction only:\n",
    "\n",
    "xr_height.mean(dim='x')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To think about the math we're going to create a new DataArray that only has a spatial dimension but no time. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# creating a linear offset\n",
    "\n",
    "height_offset = xr.DataArray(.05*space, dims=(\"x\",), coords={\"x\": space})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "height_offset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "xr_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What should adding these two do? Adding the arrays would crash and burn \n",
    "# because they have different numbers of dimension\n",
    "\n",
    "new_height = xr_height + height_offset\n",
    "new_height"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Huh? This worked? What did it do? \n",
    "\n",
    "# Here's the original\n",
    "plt.imshow(xr_height, cmap='jet')\n",
    "plt.colorbar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here's the new one.\n",
    "plt.imshow(new_height, cmap='jet')\n",
    "plt.colorbar()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**What did `+` do?**\n",
    "\n",
    "It added the offset to the `x` dimension and did it for each `t`! This makes sense!\n",
    "\n",
    "There is a lot more here, but the key is that math can be simpler. For example, it uses the coordinate information for proper matrix multiplication:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Note that `x` is the shared dimension so it should disappear after matrix\n",
    "# multiplication. \n",
    "\n",
    "xr_height @ height_offset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.5 Loading / saving `xarray` objects: NetCDF, JSON, Pickle\n",
    "\n",
    "`xarray` natively saves to the [NetCDF (Network Common Data Format)](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#What-Is-netCDF). This is a format that was originally created for climate science. Under the hood it is using a more general format which we will talk more about [Hierarchical Data Format (HDF)](https://portal.hdfgroup.org/display/HDF5/HDF5).\n",
    "\n",
    "Can convert a `DataArray` to a dictionary and save it using pythons `pickle` package or as a `JSON` file. `JSON` stands for a  JavasSript Object Notation (JSON) it's a text format for saving key value pairs (akin to a Python dictionary). [JSON website](https://www.json.org/)\n",
    "\n",
    "Can also convert it to `pandas` variables (something we'll talk about a bit later), but pandas knows how to save to **many** other formats. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 1, Saving the array as a NetCDF file...\n",
    "# xr_height.to_netcdf(\"data/height.nc\"), format='NETCDF4', engine='netcdf4')\n",
    "\n",
    "import os\n",
    "\n",
    "xr_height.to_netcdf(os.path.join(\"data\", \"height.nc\"), format='NETCDF4', engine='netcdf4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ASIDE: It's secretly an HDF5 file\n",
    "# The 'r' here stands for 'read'. Could have just as easily been 'w' for write or 'a' for append. \n",
    "\n",
    "import h5py\n",
    "\n",
    "with h5py.File(os.path.join(\"data\", \"height.nc\"), 'r') as h5_height:\n",
    "    for name in h5_height:\n",
    "        print(name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ...and checking that this worked.\n",
    "\n",
    "xr.open_dataarray(os.path.join(\"data\", \"height.nc\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 2, converting to a dictionary...\n",
    "\n",
    "dict_height = xr_height.to_dict()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_height.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ...saving dictionary as a pickle...\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(\"data\", \"height.pickle\"), 'wb') as f:\n",
    "    pickle.dump(dict_height, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ...and checking whether it worked. \n",
    "\n",
    "with open(os.path.join(\"data\", \"height.pickle\"), 'rb') as f:\n",
    "    dict_height_2 = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_height_2.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 3, converting to a dictionary and saving as JSON\n",
    "\n",
    "import json\n",
    "\n",
    "with open(os.path.join(\"data\", \"height.json\"), 'w') as f:\n",
    "    json.dump(dict_height, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(os.path.join(\"data\", \"height.json\"), 'r') as f:\n",
    "    dict_height_2 = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_height_2.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 HDF5 using `h5py`\n",
    "\n",
    "Heirarchical data format is a fomat for storing data in groups with appended metadata. [The docs for h5py can be found here.](https://docs.h5py.org/en/stable/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import h5py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Loading HDF5 files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importing the HDF5 file into python as `data`.\n",
    "# The 'r' here stands for 'read'. Could have just as easily been 'w' for write or 'a' for append. \n",
    "\n",
    "data = h5py.File(os.path.join(\"data\", \"dataset_2017_08_25_postrun\", \"2017-08-25_09-50-43.hdf5\"), 'r')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# To see all the data use the .visit function \n",
    "\n",
    "data.visit(print)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Navigating HDF5 files <a name=\"HDF\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spikes = data[\"ephys/TT1/spikes/times\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What's going on here?\n",
    "\n",
    "spikes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Need to cast to array\n",
    "\n",
    "spikes_arr = np.array(spikes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spikes_arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(spikes_arr[:100], 100*[1], c='k', marker='|')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Writing to HD5 files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# `a` = append\n",
    "\n",
    "h5file = h5py.File(os.path.join(\"data\", \"my_data.h5\"), mode=\"a\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a group\n",
    "\n",
    "group = h5file.create_group(\"/cell_1\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Making some fake data \n",
    "\n",
    "time = np.linspace(0, 100, 1000)\n",
    "voltage = np.random.rand(1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving these as arrays in the hdf5 file\n",
    "\n",
    "group.create_dataset(\"voltage\", data=voltage)\n",
    "group[\"voltage\"].attrs[\"units\"] = \"mV\"\n",
    "group.create_dataset(\"time\", data=time)\n",
    "group[\"time\"].attrs[\"units\"] = \"s\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h5file.visit(print)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Don't forget to close a file when you are done!\n",
    "\n",
    "h5file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2(alt) HDF5 using `tables` <a name=\"HDF5alt\">\n",
    "\n",
    "The `pytables` package is a  robust way to interact with HDF5 files. [You can find the package website here](https://www.pytables.org)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tables"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.1 Loading HDF5 files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# importing the HDF5 file into python as `data`.\n",
    "# The 'r' here stands for 'read'. Could have just as easily been 'w' for write or 'a' for append. \n",
    "\n",
    "data = tables.File(\"data/dataset_2017_08_25_postrun/2017-08-25_09-50-43.hdf5\", 'r')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# To have tables spit out the whole kit and caboodle  \n",
    "\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data[\"ephys/TT24/spikes\"].visit(print)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2 Navigating HDF5 files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.root.?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spikes = data.root.ephys.TT1.spikes.times"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.scatter(spikes[:100], 100*[1], s='|')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2.3 Writing to HD5 files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h5file = tables.open_file(\"data/my_data.h5\", mode=\"a\", title=\"My test file\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a group\n",
    "\n",
    "group = h5file.create_group(\"/\", 'cell_1', 'Data from cell_1')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Making some fake data \n",
    "\n",
    "time = np.linspace(0, 100, 1000)\n",
    "voltage = np.random.rand(1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# saving these as arrays in the hdf5 file\n",
    "\n",
    "h5file.create_array(group, 'time', time, \"time (ms)\")\n",
    "h5file.create_array(group, 'voltage', voltage, \"voltage (mV)\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h5file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Can also create tables with columns of different types\n",
    "# Definiing the information you'd like in your table, and creating an empty table\n",
    "\n",
    "class my_ephys(tables.IsDescription):\n",
    "    time  = tables.Float32Col()      # Signed 64-bit integ\n",
    "    voltage = tables.Float32Col()\n",
    "    \n",
    "table = h5file.create_table(group, 'my_data', my_ephys, \"recording session\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "moment = table.row\n",
    "for i, j in zip(time, voltage):\n",
    "    moment['time']  = i\n",
    "    moment['voltage'] = j\n",
    "    moment.append()\n",
    "\n",
    "table.flush()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "h5file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Saving metadata\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 DataFrames from `pandas` <a name=\"pandas\">\n",
    "\n",
    "We're going to be working through [the `pandas` tutorials here.](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.1 Creating a `DataFrame` object "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 1: Feed `DataFrame` a dictionary where keys are columns and values are a list of\n",
    "# values\n",
    "\n",
    "dict_data = {\"Name\": [\"Braund, Mr. Owen Harris\",  \"Allen, Mr. William Henry\", \"Bonnell, Miss. Elizabeth\"],\n",
    "        \"Age\": [22, 35, 58], \"Sex\": [\"male\", \"male\", \"female\"]}\n",
    "\n",
    "df = pd.DataFrame(dict_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 2, a list of dictionary objects, note what it does for the missing \n",
    "# \"sex\" for \"Owen\"\n",
    "\n",
    "option_2 = [{\"Name\": \"Owen\", \"Age\": 22}, {\"Name\": \"William\", \"Age\": 35, \"Sex\":'male'}]\n",
    "pd.DataFrame(option_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Option 3, just go directly to a dataframe from another object type...\n",
    "\n",
    "xr_height.to_pandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3.2 Working through the [pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('py3-qn': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "7eea63c48bb6deff80fa0e0a8d9fb4146f53da33c53d4c57edf67ddf53207ee0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}