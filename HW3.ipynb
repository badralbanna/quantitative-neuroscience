{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Representations of Uncertainty, Correlations, & Causal Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "\n",
    "from causalgraphicalmodels import CausalGraphicalModel\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.1: Practice different represtations of uncertainty\n",
    "\n",
    "In out discussion of uncertainty I showed you a plot which showed you the same coninious numerical data represented a number of different ways to emphasize how these different represetions capture (or suppress) certain information about the data set. In this problem, I'd like you to generate one like the one I showed from the numpy array `data_w_outliers` below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_outliers = np.hstack([ np.random.normal(loc=2, scale=1.5, size=4), np.random.normal(loc=5, scale=2, size=20), np.random.normal(loc=7, scale=1, size=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one axes with \n",
    "# 1. All data points with a marker represeting the median\n",
    "# 2. Violin plot with a marker represneting the median (see `violinplot` in matplotlib)\n",
    "# 3. A box plot \n",
    "# 4. Median and 95% confidence interval \n",
    "# 5. Median and interquartile range (IQR)\n",
    "# 6. Mean and standard deviation \n",
    "# 7. Mean and SEM (remember SEM = (std. dev.) / sqrt(n))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.2: Comparing Pearson and Spearman correlation\n",
    "\n",
    "In class we discussed the distinctions between spearman and pearson correlation. In this problem I'd like you to generate some data (`X` and `Y` below) that you think will appear more strongly correlated under spearman than pearson correlation. You'll then plot the data and discuss why you think this is the case and calculate these quantities using whichever method you choose to verify this directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "X = ???\n",
    "Y = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and comment on which correlation measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson and Spearman "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.3: Finish the CramersV function from class\n",
    "\n",
    "In class, we discussed the Cramer's V correlation for nominal data. In this problem you'll make a function using a built in `scipy` $\\chi^2$ function that implements this forumla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a random contingincy table \n",
    "\n",
    "data = np.random.randint(1, 50, size=(3,2))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a table based on that table that is generated independantly (i.e. it should have a low correlation)\n",
    "\n",
    "n = data.sum()\n",
    "n0 = data.sum(axis=1)\n",
    "p0 = (n0 + 1) / (n + len(n0))\n",
    "n1 = data.sum(axis=0)\n",
    "p1 = (n1 + 1) / (n + len(n1))\n",
    "\n",
    "data_indep = np.array([[int(n*p0_val*p1_val) for p1_val in p1] for p0_val in p0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scipy function to calculate chi2 and use it on both data sets above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how to derive the remaining values from `data` or `data_indep` that you need to calculate Cramer's V (i.e. total number of counts, n, and minimum dimension size - 1). Use these and chi2 above to write your own Cramer's V function \n",
    "\n",
    "n = ???\n",
    "min_dim = ???\n",
    "\n",
    "def CramersV(data, ...):\n",
    "    ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Turns out the \"naive\" forumla for Cramers V is biased. Look up the correction and modify your function to include it.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.4: Experiments with Causal Modeling\n",
    "\n",
    "In this problem, you'll be playing out with the tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.4.1 "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8427839d926cfa02cf98146f41fa7aa1f3aedf26d017c65310c4e12a8a7b9def"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('py3-qn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
